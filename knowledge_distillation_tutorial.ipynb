{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYDFa1TX7PaH"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1.keras import backend as tfk\n",
    "from tensorflow.compat.v1.keras.initializers import RandomNormal, TruncatedNormal\n",
    "from tensorflow.compat.v1.keras.layers import (Input, Dense, Activation, Layer, Lambda,\n",
    "                                     Concatenate)\n",
    "from tensorflow.compat.v1.keras.models import Model, load_model\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "\n",
    "from models import ConvNet, SoftBinaryDecisionTree\n",
    "from models.utils import brand_new_tfsession, draw_tree\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "sess = brand_new_tfsession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aemgMmx07PaM"
   },
   "source": [
    "## Zad. 1 \n",
    "Podział danych na zbiory treningowe, walidacyjne i testowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEInh49Q7PaN",
    "outputId": "5ba3aee8-fb54-499a-e9a7-85aade3ec249"
   },
   "outputs": [],
   "source": [
    "# load MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# add channel dim\n",
    "x_train, x_test = x_train[..., np.newaxis], x_test[..., np.newaxis]\n",
    "\n",
    "# hold out last 10000 training samples for validation\n",
    "# x_valid, y_valid =  # TODO\n",
    "# x_train, y_train =  # TODO\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape)\n",
    "# (50000, 28, 28, 1) (50000,) (10000, 28, 28, 1) (10000,) (10000, 28, 28, 1) (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqMeTokN7PaO",
    "outputId": "e73eca75-8913-4de1-a732-a9eaacf22d30"
   },
   "outputs": [],
   "source": [
    "# retrieve image and label shapes from training data\n",
    "img_rows, img_cols, img_chans = x_train.shape[1:]\n",
    "n_classes = np.unique(y_train).shape[0]\n",
    "\n",
    "print(img_rows, img_cols, img_chans, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Lf7wzGv7PaQ",
    "outputId": "967a38b1-6616-4fba-a622-502ea879f68f"
   },
   "outputs": [],
   "source": [
    "# convert labels to 1-hot vectors\n",
    "y_train = tf.keras.utils.to_categorical(y_train, n_classes)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, n_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8Nb0e407PaR"
   },
   "outputs": [],
   "source": [
    "# normalize inputs and cast to float\n",
    "x_train = (x_train / np.max(x_train)).astype(np.float32)\n",
    "x_valid = (x_valid / np.max(x_valid)).astype(np.float32)\n",
    "x_test = (x_test / np.max(x_test)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLAsTW8J7PaR"
   },
   "source": [
    "### Neural Network\n",
    "##### Sieci neuronowe jako model nauczyciela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzjarEF17PaS"
   },
   "outputs": [],
   "source": [
    "nn = ConvNet(img_rows, img_cols, img_chans, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlsY2kAf7PaU",
    "outputId": "43554c1d-09fe-411d-e34c-0769499c5cb0"
   },
   "outputs": [],
   "source": [
    "nn = ConvNet(img_rows, img_cols, img_chans, n_classes)\n",
    "nn.maybe_train(data_train=(x_train, y_train),\n",
    "               data_valid=(x_valid, y_valid),\n",
    "               batch_size=16, epochs=12, model_name='nn-model')\n",
    "# NOTE: if the model doesn't load properly try model_name='nn-model-alternative'\n",
    "nn.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FU3-jTrS7PaW",
    "outputId": "8bdc365d-af39-4c79-adb2-247ced64b49d"
   },
   "outputs": [],
   "source": [
    "nn.evaluate(x_valid, y_valid)\n",
    "nn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 2 Extraction of soft labels for distillation\n",
    "Wyekstrahować `soft labels` potrzebne do destylacji. Trzeba wykorzystać metodę klasy `ConvNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuSVEl5Q7PaW",
    "outputId": "d57a2625-10de-484a-f0e2-eb5247edb79f"
   },
   "outputs": [],
   "source": [
    "# y_train_soft =  # TODO\n",
    "y_train_soft.shape # (50000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkOCDuHk7PaX"
   },
   "source": [
    "## Zad. 3 Binary Soft Decision Tree\n",
    "Należy wypłaszczyć zbiór danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEUBEEGf7PaY",
    "outputId": "8e1ba9de-f542-49d9-f6b2-f7509a0bcac4"
   },
   "outputs": [],
   "source": [
    "# x_train_flat =  # TODO\n",
    "# x_valid_flat =  # TODO\n",
    "# x_test_flat =  # TODO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(x_test_flat.reshape((x_test_flat.shape[0], img_rows, img_cols))[1])\n",
    "\n",
    "x_train_flat.shape, x_valid_flat.shape, x_test_flat.shape  # ((50000, 784), (10000, 784), (10000, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzp2DA6T7PaZ"
   },
   "source": [
    "<a id='hyperparameters'></a>\n",
    "### Hyperparameters\n",
    "* `tree_depth`: as denoted in the [[paper](https://arxiv.org/pdf/1711.09784.pdf)], depth is in terms of inner nodes (excluding leaves / indexing depth from `0`)\n",
    "* `penalty_strength`: regularization penalty strength\n",
    "* `penalty_decay`: regularization penalty decay: paper authors found 0.5 optimal (note that $2^{-d} = 0.5^d$ as we use it)\n",
    "* `ema_win_size`: scaling factor to the \"default size of the window\" used to calculate moving averages (growing exponentially with depth) of node and path probabilities\n",
    "* `inv_temp`: scale logits of inner nodes to \"avoid very soft decisions\" [[paper](https://arxiv.org/pdf/1711.09784.pdf)]\n",
    "    * pass `0` to indicate that this should be a learned parameter (single scalar learned to apply to all nodes in the tree)\n",
    "* `learning_rate`: hopefully no need to explain, but let's be cool and use [Karpathy constant](https://www.urbandictionary.com/define.php?term=Karpathy%20Constant) ([source](https://twitter.com/karpathy/status/801621764144971776)) :D as default in `tree.__init__()`\n",
    "* `batch_size`: we use a small one, because with increasing depth and thus amount of leaf bigots, larger batch sizes cause their loss terms to be scaled down too much by averaging, which results in poor optimization properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0j9GpxS7PaZ"
   },
   "outputs": [],
   "source": [
    "n_features = img_rows * img_cols * img_chans\n",
    "tree_depth = 4\n",
    "penalty_strength = 1e+1\n",
    "penalty_decay = 0.25\n",
    "ema_win_size = 1000\n",
    "inv_temp = 0.01\n",
    "learning_rate = 5e-03\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ewumqmcc7PaZ"
   },
   "source": [
    "### Regular training with hard labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMVKGLXN7Paa",
    "outputId": "5199cf59-785c-4542-aabd-e9a621cae981"
   },
   "outputs": [],
   "source": [
    "sess = brand_new_tfsession(sess)\n",
    "\n",
    "tree = SoftBinaryDecisionTree(tree_depth, n_features, n_classes,\n",
    "    penalty_strength=penalty_strength, penalty_decay=penalty_decay,\n",
    "    inv_temp=inv_temp, ema_win_size=ema_win_size, learning_rate=learning_rate)\n",
    "tree.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VycdC_JV7Paa",
    "outputId": "43294d8b-d0c8-4e94-f480-bcdb44040b51"
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', patience=20, verbose=1)\n",
    "\n",
    "tree.maybe_train(\n",
    "    sess=sess, data_train=(x_train_flat, y_train), data_valid=(x_valid_flat, y_valid),\n",
    "    batch_size=batch_size, epochs=epochs, callbacks=[es], distill=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C35zwVVv7Pab",
    "outputId": "9c6f8cf2-f9ab-44db-9c71-7698cc97e2da"
   },
   "outputs": [],
   "source": [
    "tree.evaluate(x=x_valid_flat, y=y_valid, batch_size=batch_size)\n",
    "tree.evaluate(x=x_test_flat, y=y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58tJzTv67Pac"
   },
   "source": [
    "### Distillation: training with soft labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yC6phws7Pac",
    "outputId": "ae4d5f03-4593-4881-a396-6e670b41f294"
   },
   "outputs": [],
   "source": [
    "sess = brand_new_tfsession(sess)\n",
    "\n",
    "tree = SoftBinaryDecisionTree(tree_depth, n_features, n_classes,\n",
    "    penalty_strength=penalty_strength, penalty_decay=penalty_decay,\n",
    "    inv_temp=inv_temp, ema_win_size=ema_win_size, learning_rate=learning_rate)\n",
    "tree.build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 4\n",
    "Wytrenuj drzewo z wykorzystaniem destylacji oraz `soft labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b80XBn17Pad",
    "outputId": "bb69a3ec-3b82-4e81-f84a-a75d39ce70a7"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', patience=20, verbose=1)\n",
    "\n",
    "# TODO: wykorzystaj tej samej metody co w przypadku zwykłego drzewa (powyżej). Wykorzystaj również `soft labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d33e5dAz7Pad",
    "outputId": "27b2e415-ae1b-436d-f005-ae36daea0ce3"
   },
   "outputs": [],
   "source": [
    "tree.evaluate(x=x_valid_flat, y=y_valid, batch_size=batch_size)\n",
    "tree.evaluate(x=x_test_flat, y=y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Wizualizacja'></a>\n",
    "Zwizualizujmy sobie teraz jak drzewo rozpoznaje różne cyfry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "for digit in range(10):\n",
    "    sample_index = np.random.choice(np.where(np.argmax(y_test, axis=1)==digit)[0])\n",
    "    input_img = x_test[sample_index]\n",
    "    draw_tree(sess, tree, img_rows, img_cols, img_chans, input_img=input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytanie kontrolne\n",
    "##### Które z drzew daje lepsze rezultaty i o ile?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad. 5\n",
    "Wytrenuj drzewa o głębokości `tree_depth=[1, 2, 3, 5]` (dla `soft labels` oraz `hard labels`) i porównaj otrzymane rezultaty\n",
    "z wcześniejszymi wynikami dla głębokości `tree_depth=4`. \n",
    "Można użyć funkcji [`draw_tree`](#Wizualizacja) (z zadania 4) dla każdego drzewa w celu zwizualizowania jego węzłów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie dodatkowe:\n",
    "Znaleźć optymalne wartości parametrów dla drzew decyzyjnych w przypadku zarówno `soft labels` jak i `hard labels` (modyfikując wartości parametrów opisanych w sekcji [Hyperparameters](#hyperparameters)). Przetestować drzewa dla większych wartości `tree_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "mnist2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
